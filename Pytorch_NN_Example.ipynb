{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_NN_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VaiSuliafu/CS6350_MachineLearning/blob/master/Pytorch_NN_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzBeAJAjI1Ej"
      },
      "source": [
        "# imports\n",
        "import torch # entire library\n",
        "import torch.nn as nn # nn modules and loss functions\n",
        "import torch.optim as optim # optimization algorithms\n",
        "import torch.nn.functional as F # functions without paramters (activation functions)\n",
        "from torch.utils.data import DataLoader, Dataset # DataLoader gives us easier dataset management \n",
        "import torchvision.datasets as datasets # pytorch standard datasets\n",
        "import torchvision.transforms as transforms # transformations we can perform on our dataset\n",
        "\n",
        "import requests # for getting M4 testing/training dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from fbprophet import Prophet\n",
        "\n",
        "import datetime\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsqRbevIJExz"
      },
      "source": [
        "# This section holds a class for each of the models to be tested\n",
        "\n",
        "# Create Fully Connected Network\n",
        "class NN(nn.Module):\n",
        "  def __init__(self, input_size, num_classes):\n",
        "    super(NN, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 50)\n",
        "    self.fc2 = nn.Linear(50, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "# Create CNN \n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, in_channel = 1, num_classes = 10):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=(1,1)) # same convolution\n",
        "    self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1)) # same convolution \n",
        "    self.fc1 = nn.Linear(16*7*7, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Create an RNN\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # initial hidden state\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "    # Forward Prop\n",
        "    out, _ = self.rnn(x, h0)\n",
        "    out = out.reshape(out.shape[0], -1)\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "# Create a GRU\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "    super(GRU, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # initial hidden state\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "    # Forward Prop\n",
        "    out, _ = self.gru(x, h0)\n",
        "    out = out.reshape(out.shape[0], -1)\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "# Create an LSTM\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # initial hidden state\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "    # Forward Prop\n",
        "    out, _ = self.lstm(x, (h0, c0))\n",
        "    out = out.reshape(out.shape[0], -1)\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "# Bi-directional LSTM\n",
        "class BRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "    super(BRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "    self.fc = nn.Linear(hidden_size*2, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "    out, _ = self.lstm(x, (h0, c0))\n",
        "    out = self.fc(out[:, -1, :])\n",
        "\n",
        "    return out\n",
        "\n",
        "# Momentum LSTM Cell\n",
        "class MomentumLSTMCell(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, mu, epsilon, bias=True, fg_init=1.0):\n",
        "    super(MomentumLSTMCell, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size =  hidden_size\n",
        "    self.bias = bias\n",
        "    self.fg_init = fg_init\n",
        "    self.x2h = nn.Linear(input_size, 4 * hidden_size, bias=bias)\n",
        "    self.h2h = nn.Linear(hidden_size, 4 * hidden_size, bias=bias)\n",
        "\n",
        "    # for momentum net\n",
        "    self.mu = mu\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.reset_parameters(hidden_size)\n",
        "\n",
        "  def reset_parameters(self, hidden_size):\n",
        "    nn.init.orthogonal_(self.x2h.weight)\n",
        "    nn.init.eye_(self.h2h.weight)\n",
        "    nn.init.zeros_(self.x2h.bias)\n",
        "    self.x2h.bias.data[hidden_size:(2 * hidden_size)].fill_(self.fg_init)\n",
        "    nn.init.zeros_(self.h2h.bias)\n",
        "    self.h2h.bias.data[hidden_size:(2 * hidden_size)].fill_(self.fg_init)\n",
        "\n",
        "  def forward(self, x, hidden, v):\n",
        "\n",
        "    hx, cx = hidden\n",
        "    x = x.squeeze()\n",
        "    x = x.view(-1, x.size(1))\n",
        "    v = v.view(-1, v.size(1))\n",
        "\n",
        "    vy = self.mu * v + self.epsilon * self.x2h(x)\n",
        "    \n",
        "    gates = vy + self.h2h(hx).squeeze()\n",
        "    gates = gates.squeeze()\n",
        "\n",
        "    i,f,o,g = gates.chunk(4, 1)\n",
        "    i = torch.sigmoid(i)\n",
        "    f = torch.sigmoid(f)\n",
        "    o = torch.sigmoid(o)\n",
        "    g = torch.tanh(g)\n",
        "\n",
        "    cy = (cx * f) + (i * g)\n",
        "    hy = o * torch.tanh(cy)\n",
        "\n",
        "    return hy, (hy, cy), vy\n",
        "\n",
        "# Create a MomentumLSTM\n",
        "class MomentumLSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, mu, epsilon, bias=True, fg_init=1.0):\n",
        "    super(MomentumLSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.mlstm = MomentumLSTMCell(input_size, hidden_size, mu, epsilon, bias, fg_init)\n",
        "    self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # initial hidden state\n",
        "    h0 = torch.zeros(x.size(0)*x.shape[2], self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(x.size(0)*x.shape[2], self.hidden_size).to(device)\n",
        "    v = torch.zeros(x.shape[0]*x.shape[2], 4 * self.hidden_size).to(device)\n",
        "\n",
        "    # Forward Prop\n",
        "    out, _, _ = self.mlstm(x, (h0, c0), v)\n",
        "    out = out.reshape(x.shape[0], -1)\n",
        "    out = self.fc(out)\n",
        "    return out"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rTw1GfKv2Y9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48880f9d-da79-47fe-9afa-6805033b87fd"
      },
      "source": [
        "!pip install pytorch-forecasting\r\n",
        "from pytorch_forecasting.metrics import MAPE"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-forecasting\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b4/a2f5b718022a92ce1aca3635f8a037b10da8d9f0202b49b5dac6442b4efb/pytorch_forecasting-0.7.1-py3-none-any.whl (83kB)\n",
            "\r\u001b[K     |████                            | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 40kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pytorch-forecasting) (3.2.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from pytorch-forecasting) (0.10.2)\n",
            "Requirement already satisfied: torch<2.0,>=1.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-forecasting) (1.7.0+cu101)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-forecasting) (1.1.4)\n",
            "Collecting optuna<3.0.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/10/06b58f4120f26b603d905a594650440ea1fd74476b8b360dbf01e111469b/optuna-2.3.0.tar.gz (258kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 9.4MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn<0.24,>=0.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pytorch-forecasting) (1.4.1)\n",
            "Collecting pytorch-lightning<2.0.0,>=1.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/d0/84a2f072cd407f93a1e50dff059656bce305f084e63a45cbbceb2fdb67b4/pytorch_lightning-1.1.0-py3-none-any.whl (665kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 30.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from pytorch-forecasting) (1.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pytorch-forecasting) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pytorch-forecasting) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pytorch-forecasting) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pytorch-forecasting) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pytorch-forecasting) (1.18.5)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->pytorch-forecasting) (0.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2.0,>=1.4->pytorch-forecasting) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<2.0,>=1.4->pytorch-forecasting) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch<2.0,>=1.4->pytorch-forecasting) (0.8)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<2.0.0,>=1.1.0->pytorch-forecasting) (2018.9)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/c8/c16d30bbed11a1722060014c246d124582d1f781b26f5859d8dacc3e08e1/colorlog-4.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.3.20)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/61/5b64d73b01c1218f55c894b5ec0fb89b32c6960b7f7b3ad9f5ac0c373b9d/cliff-3.5.0-py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.8MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/3c/06c76ec8b54b9b1fad7f35e903fd25010fe3e0d41bd94cea5e6f12e0d651/cmaes-0.7.0-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (0.17.0)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 39.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna<3.0.0,>=2.3.0->pytorch-forecasting) (20.4)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 41.5MB/s \n",
            "\u001b[?25hCollecting fsspec>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->pytorch-forecasting) (1.15.0)\n",
            "Collecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/54/af6e2703f064485d717cb311d3f9440cd302a823ba6d80a020b59eae166d/cmd2-1.4.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 28.9MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hCollecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 35.6MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.7MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.7.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.33.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.17.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.4.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (2.23.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/4c/0b1d507ad7e8bc31d690d04b4f475e74c2002d060f7994ce8c09612df707/pyperclip-1.8.1.tar.gz\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (2.0.0)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (20.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (1.1.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting) (3.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<2.0.0,>=1.0.4->pytorch-forecasting) (3.1.0)\n",
            "Building wheels for collected packages: optuna\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-cp36-none-any.whl size=359761 sha256=c3abc764db4864a485c915d42c7e99d10d33ac9c80521b1056e1b50849f11ef0\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/91/19/64b0ec6b964f89c0695a9dc6db6f851d0b54c5381a5c9cadfb\n",
            "Successfully built optuna\n",
            "Building wheels for collected packages: PyYAML, PrettyTable, pyperclip\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=85e08107357f30d1da83d3fe67b9f6b0cc873fa4b770aa6cdfac59bfc10ad1cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-cp36-none-any.whl size=13700 sha256=c0fff48f17ba66becc2dc8865efd9c72390830b644daf21eee9536e9e4069689\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.1-cp36-none-any.whl size=11119 sha256=cfe834c2423d772c7d493fed7e6277021b221e00b5a87d3a18b90974c6d459c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/10/3a/c830e9bb3db2c93274ea1f213a41fabde0d8cf3794251fad0c\n",
            "Successfully built PyYAML PrettyTable pyperclip\n",
            "\u001b[31mERROR: pytorch-lightning 1.1.0 has requirement future>=0.17.1, but you'll have future 0.16.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: colorlog, pyperclip, colorama, cmd2, pbr, stevedore, PrettyTable, PyYAML, cliff, cmaes, Mako, python-editor, alembic, optuna, threadpoolctl, scikit-learn, fsspec, pytorch-lightning, pytorch-forecasting\n",
            "  Found existing installation: prettytable 2.0.0\n",
            "    Uninstalling prettytable-2.0.0:\n",
            "      Successfully uninstalled prettytable-2.0.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed Mako-1.1.3 PrettyTable-0.7.2 PyYAML-5.3.1 alembic-1.4.3 cliff-3.5.0 cmaes-0.7.0 cmd2-1.4.0 colorama-0.4.4 colorlog-4.6.2 fsspec-0.8.4 optuna-2.3.0 pbr-5.5.1 pyperclip-1.8.1 python-editor-1.0.4 pytorch-forecasting-0.7.1 pytorch-lightning-1.1.0 scikit-learn-0.23.2 stevedore-3.3.0 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xPSm4Ro4zyb"
      },
      "source": [
        "# function for saving a trained model\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"=> Saving checkpoint\")\n",
        "  torch.save(state, filename)\n",
        "\n",
        "# function for loading a saved model\n",
        "def load_checkpoint(check_point):\n",
        "  print(\"=> Loading checkpoint\")\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfILzU3iyvip"
      },
      "source": [
        "# Custom Dataset for Dataloader & Mini batch compatability\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, window, target_cols):\n",
        "        self.data = torch.Tensor(data)\n",
        "        self.window = window\n",
        "        self.target_cols = target_cols\n",
        "        self.shape = self.__getshape__()\n",
        "        self.size = self.__getsize__()\n",
        " \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index:index+self.window]\n",
        "        y = self.data[index+self.window,0:self.target_cols]\n",
        "        return x, y\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.data) -  self.window \n",
        "    \n",
        "    def __getshape__(self):\n",
        "        return (self.__len__(), *self.__getitem__(0)[0].shape)\n",
        "    \n",
        "    def __getsize__(self):\n",
        "        return (self.__len__())\n",
        "\n",
        "# function to load the m4 train/test data for a specified period length\n",
        "def load_m4_data(time = \"Daily\", train=True):\n",
        "\n",
        "  if train:\n",
        "    url = 'https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Train/' + time + '-train.csv'\n",
        "    filename = time.lower() + '_train.csv'\n",
        "  else:\n",
        "    url = 'https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Test/' + time + '-test.csv'\n",
        "    filename = time.lower() + '_test.csv'\n",
        "\n",
        "  res = requests.get(url, allow_redirects=True)\n",
        "\n",
        "  with open(filename,'wb') as file:\n",
        "      file.write(res.content)\n",
        "  df = pd.read_csv(filename, index_col=0)\n",
        "\n",
        "  return df\n",
        "\n",
        "# function to compute loss on test set\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# function to make predictions for test set\n",
        "def make_pred(model, lastTrain, predSize=14):\n",
        "  with torch.no_grad():\n",
        "    test_seq = lastTrain\n",
        "    np.zeros(predSize)\n",
        "    for i in range(predSize):\n",
        "      y_test_pred = model(test_seq)\n",
        "      pred = torch.flatten(y_test_pred).item()\n",
        "      preds[i] = [pred]\n",
        "      new_seq = test_seq.numpy().flatten()\n",
        "      new_seq = np.append(new_seq, [pred])\n",
        "      new_seq = new_seq[1:]\n",
        "      test_seq = torch.as_tensor(new_seq).view(1, sequence_length, 1).float()\n",
        "  return np.array(preds)\n",
        "\n",
        "# prints the dimensions of a dataset\n",
        "def print_dims(df, name):\n",
        "  print(\"{} shape = {}\".format(name, df.shape))"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB4gQhQxUDAT",
        "outputId": "0668d680-c98c-4d87-d18b-de90aca8b606"
      },
      "source": [
        "# load daily training data\r\n",
        "df_train = load_m4_data(\"Daily\", train=True)\r\n",
        "\r\n",
        "# load daily testing data\r\n",
        "df_test = load_m4_data(\"Daily\", train=False)\r\n",
        "\r\n",
        "# print dimensions\r\n",
        "print_dims(df_train, \"Daily_train\")\r\n",
        "print_dims(df_test, \"Daily_test\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Daily_train shape = (4227, 9919)\n",
            "Daily_test shape = (4227, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhdNPHxaKsoT",
        "outputId": "fec24a57-9f7f-43b3-feb3-a1d59ceab746"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# Hyperparameters\n",
        "in_channel = 1 # CNN\n",
        "# input_size = 784 # FCN\n",
        "input_size = 1 # RNN\n",
        "sequence_length = 28 # RNN\n",
        "num_layers = 2\n",
        "hidden_size = 256\n",
        "num_classes = 1\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 2\n",
        "load_model = False\n",
        "# mu = 0.6 # MomentumLSTM\n",
        "# epsilon = 0.6 # MomentumLSTM"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_B98T750sf0",
        "outputId": "029083ce-d137-453c-c425-d1b9fbf66cb6"
      },
      "source": [
        "# extract series\r\n",
        "series = df_train.iloc[0, :]\r\n",
        "series.dropna(inplace=True)\r\n",
        "\r\n",
        "# Expand the dimension from [N,] => [N,1]\r\n",
        "series = np.expand_dims(series, axis=1)\r\n",
        "labels = np.expand_dims(df_test.iloc[0, :], axis=1)\r\n",
        "\r\n",
        "# Fit a min max scaler to this series\r\n",
        "scaler = MinMaxScaler()\r\n",
        "scaler = scaler.fit(series)\r\n",
        "\r\n",
        "# Transform the series and labels\r\n",
        "train_data = scaler.transform(series)\r\n",
        "test_data = scaler.transform(labels)\r\n",
        "\r\n",
        "train_dataset = MyDataset(data=train_data, window=sequence_length, target_cols=1)\r\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\r\n",
        "\r\n",
        "print(train_dataset.shape)\r\n",
        "print(test_data.shape)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(978, 28, 1)\n",
            "(14, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1ob4HHbMAEa"
      },
      "source": [
        "# Initialize Network\n",
        "# model = NN(input_size=input_size, num_classes=num_classes).to(device)\n",
        "# model = CNN(in_channel=in_channel, num_classes=num_classes).to(device)\n",
        "# model = RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "# model = GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "model = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "# model = BRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes).to(device)\n",
        "# model = MomentumLSTM(input_size=input_size, hidden_size=hidden_size, mu=mu, epsilon=epsilon).to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = MAPE()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "if load_model:\n",
        "  load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"))"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzjJiYuBNalb"
      },
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, FCN=False):\n",
        "  # Load trained model if one exists\n",
        "  if load_model:\n",
        "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"))\n",
        "\n",
        "  # Train Network\n",
        "  for epoch in range(10):\n",
        "    losses = []\n",
        "\n",
        "    if load_model:\n",
        "      if epoch % 100 == 0:\n",
        "        checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "        save_checkpoint(checkpoint)\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "      if not FCN:\n",
        "        data = data.to(device=device).squeeze(1) # ON FOR RNN/LSTM compatibility\n",
        "      data = data.to(device=device)\n",
        "      targets = targets.to(device=device)\n",
        "      \n",
        "      # flatten tensor to correct shape\n",
        "      if FCN:\n",
        "        data = data.reshape(data.shape[0], -1) # ON FOR FCN compatability\n",
        "      \n",
        "      # forward pass\n",
        "      scores = model(data)\n",
        "      loss = criterion(scores, targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      # backward\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # gradient descent or adam step\n",
        "      optimizer.step()\n",
        "\n",
        "    mean_loss = sum(losses)/len(losses)\n",
        "    if epoch % 50 == 0:\n",
        "      print(\"Loss at epoch {} was {}\".format(epoch, mean_loss))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_8BcgC9Mpr9",
        "outputId": "377b43b8-f95a-41b3-9ebb-c08cf22cdff8"
      },
      "source": [
        "# Create rolling window data\r\n",
        "X_test, _ = sliding_windows(train_data[-29:], sequence_length)\r\n",
        "print(X_test.shape)\r\n",
        "\r\n",
        "# Create torch tensors\r\n",
        "X_test = torch.from_numpy(X_test).float().to(device)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEaiNjSbPhny",
        "outputId": "0f45b824-99c4-46bf-a2b0-ddfbcf8b9b05"
      },
      "source": [
        "preds = make_pred(model, X_test)\r\n",
        "print(mean_absolute_percentage_error(test_data, preds))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98.85925130954041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOB0T3TkXx5I"
      },
      "source": [
        "# FCN Train\n",
        "# Got 58991 / 60000 with accuracy 98.32\n",
        "# FCN Test\n",
        "# Got 9693 / 10000 with accuracy 96.93\n",
        "\n",
        "# CNN Train\n",
        "# Got 59397 / 60000 with accuracy 99.00\n",
        "# CNN Test\n",
        "# Got 9850 / 10000 with accuracy 98.50\n",
        "\n",
        "# RNN Train\n",
        "# Got 59076 / 60000 with accuracy 98.46\n",
        "# RNN Test\n",
        "# Got 9796 / 10000 with accuracy 97.96\n",
        "\n",
        "# GRU Train\n",
        "# Got 59691 / 60000 with accuracy 99.48\n",
        "# GRU Test\n",
        "# Got 9893 / 10000 with accuracy 98.93\n",
        "\n",
        "# LSTM Train\n",
        "# Got 59788 / 60000 with accuracy 99.65\n",
        "# LSTM Test\n",
        "# Got 9892 / 10000 with accuracy 98.92\n",
        "\n",
        "# BRNN Train\n",
        "# Got 59767 / 60000 with accuracy 99.61\n",
        "# BRNN Test\n",
        "# Got 9917 / 10000 with accuracy 99.17\n",
        "\n",
        "# MomentumLSTM Train\n",
        "# Got 58463 / 60000 with accuracy 97.44\n",
        "# MomentumLSTM Test\n",
        "# Got 9648 / 10000 with accuracy 96.48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhmj9fDf2LF_"
      },
      "source": [
        "Bring in data from M4 - daily data. The testing dataset is 15 days, so it looks like that will be the number of days forward we are trying to predict. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b-6YwR_oYRwQ",
        "outputId": "a01cbd08-1825-4719-d444-5dd7145c1b7b"
      },
      "source": [
        "''' this loop will control the entire procedure but it is not done so I have commented it out. I am testing the same code on a single iteration below - Vai '''\r\n",
        "\r\n",
        "# for row, series in df_train.iterrows():\r\n",
        "\r\n",
        "#   # extract series\r\n",
        "#   series = df_train.iloc[row, :]\r\n",
        "#   series.dropna(inplace=True)\r\n",
        "\r\n",
        "#   # Expand the dimension from [N,] => [N,1]\r\n",
        "#   series = np.expand_dims(series, axis=1)\r\n",
        "#   labels = np.expand_dims(df_test.iloc[row, :], axis=1)\r\n",
        "\r\n",
        "#   # Fit a min max scaler to this series\r\n",
        "#   scaler = MinMaxScaler()\r\n",
        "#   scaler = scaler.fit(series)\r\n",
        "\r\n",
        "#   # Transform train and test data with scaler\r\n",
        "#   train_data = scaler.transform(series)\r\n",
        "#   test_data = scaler.transform(labels)\r\n",
        "\r\n",
        "#   # Set window size to num of predictions \r\n",
        "#   seq_length = len(labels)\r\n",
        "\r\n",
        "#   # roll the data\r\n",
        "#   X_train, y_train = sliding_windows(train_data, seq_length)\r\n",
        "#   # X_test, y_test = sliding_windows(test_data, seq_length)\r\n",
        "\r\n",
        "#   # make tensors\r\n",
        "#   X_train = torch.from_numpy(X_train).float()\r\n",
        "#   y_train = torch.from_numpy(y_train).float()\r\n",
        "\r\n",
        "#   # X_test = torch.from_numpy(X_test).float()\r\n",
        "#   # y_test = torch.from_numpy(y_test).float()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' this loop will control the entire procedure but it is not done so I have commented it out. I am testing the same code on a single iteration below - Vai '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxcKZO6FXvi4"
      },
      "source": [
        "# returns the data in a stacked rolling window format\r\n",
        "def sliding_windows(data, seq_length):\r\n",
        "  xs = []\r\n",
        "  ys = []\r\n",
        "\r\n",
        "  for i in range(0, len(data) - seq_length):\r\n",
        "    x = data[i:(i+seq_length)]\r\n",
        "    y = data[(i+seq_length)]\r\n",
        "    xs.append(x)\r\n",
        "    ys.append(y)\r\n",
        "\r\n",
        "  return np.array(xs), np.array(ys)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4qkQz_7cJjh",
        "outputId": "19c09ab3-792a-498a-aee6-4f8330d433a9"
      },
      "source": [
        "# extract series\r\n",
        "series = df_train.iloc[0, :]\r\n",
        "series.dropna(inplace=True)\r\n",
        "\r\n",
        "# Expand the dimension from [N,] => [N,1]\r\n",
        "series = np.expand_dims(series, axis=1)\r\n",
        "labels = np.expand_dims(df_test.iloc[0, :], axis=1)\r\n",
        "\r\n",
        "# Fit a min max scaler to this series\r\n",
        "scaler = MinMaxScaler()\r\n",
        "scaler = scaler.fit(series)\r\n",
        "\r\n",
        "# Transform the series and labels\r\n",
        "train_data = scaler.transform(series)\r\n",
        "test_data = scaler.transform(labels)\r\n",
        "\r\n",
        "# Set window size to num of predictions \r\n",
        "seq_length = 14\r\n",
        "\r\n",
        "# Create rolling window data\r\n",
        "X_train, y_train = sliding_windows(train_data, seq_length, 14)\r\n",
        "y_train = y_train.squeeze()\r\n",
        "print(X_train.shape)\r\n",
        "print(y_train.shape)\r\n",
        "# X_test, y_test = sliding_windows(test_data, seq_length)\r\n",
        "\r\n",
        "# Create torch tensors\r\n",
        "X_train = torch.from_numpy(X_train).float().to(device)\r\n",
        "y_train = torch.from_numpy(y_train).float().to(device)\r\n",
        "\r\n",
        "# X_test = torch.from_numpy(X_test).float()\r\n",
        "# y_test = torch.from_numpy(y_test).float()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(964, 14, 1)\n",
            "(964, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpKpOWxpnpdH"
      },
      "source": [
        "# Create an LSTM\r\n",
        "class LSTM(nn.Module):\r\n",
        "  def __init__(self, input_size, hidden_size, seq_length, num_layers=2):\r\n",
        "    super(LSTM, self).__init__()\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.seq_length = seq_length\r\n",
        "    self.num_layers = num_layers\r\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\r\n",
        "    self.fc = nn.Linear(hidden_size, out_features=14)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    # initial hidden state\r\n",
        "    h0 = torch.zeros(self.num_layers, self.seq_length, self.hidden_size).to(device)\r\n",
        "    c0 = torch.zeros(self.num_layers, self.seq_length, self.hidden_size).to(device)\r\n",
        "\r\n",
        "    # Forward Prop\r\n",
        "    out, _ = self.lstm(x.view(x.shape[0], self.seq_length, -1), (h0, c0))\r\n",
        "    out = out.view(self.seq_length, x.shape[0], self.hidden_size)[-1]\r\n",
        "    out = self.fc(out)\r\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XVGag4cqVDI"
      },
      "source": [
        "def train_model(model, train_data, train_labels, validation_data=None, test_labels=None, num_epochs=60):\r\n",
        "\r\n",
        "  # init the loss function\r\n",
        "  loss_fn = MAPE()\r\n",
        "\r\n",
        "  # init the optimizer\r\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "  # init empty arrays to store results\r\n",
        "  train_hist = np.zeros(num_epochs)\r\n",
        "  test_hist = np.zeros(num_epochs)\r\n",
        "\r\n",
        "  # train\r\n",
        "  for t in range(num_epochs):\r\n",
        "\r\n",
        "    # make predictions\r\n",
        "    y_pred = model(X_train)\r\n",
        "\r\n",
        "    # calculate loss\r\n",
        "    loss = loss_fn(y_pred.float(), y_train)\r\n",
        "\r\n",
        "    # print loss for this epoch\r\n",
        "    if t % 20 == 0:\r\n",
        "      print(f'Epoch {t} train loss. {loss.item()}')\r\n",
        "\r\n",
        "    # record loss\r\n",
        "    train_hist[t] = loss.item()\r\n",
        "\r\n",
        "    # reset optimizer \r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # backprop\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # update model\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  return model.eval(), train_hist, test_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmV5hWg5yti0",
        "outputId": "c903143d-150c-4819-8740-d3b9420703d8"
      },
      "source": [
        "# instantiate model \r\n",
        "model = LSTM(1, 512, seq_length=seq_length, num_layers=2).to(device)\r\n",
        "\r\n",
        "# train model\r\n",
        "model, train_hist, test_hist = train_model(model, train_data=X_train, train_labels=y_train, num_epochs=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 train loss. 1959.8482666015625\n",
            "Epoch 20 train loss. 388.1073303222656\n",
            "Epoch 40 train loss. 230.57875061035156\n",
            "Epoch 60 train loss. 194.63063049316406\n",
            "Epoch 80 train loss. 124.95750427246094\n",
            "Epoch 100 train loss. 136.7980499267578\n",
            "Epoch 120 train loss. 128.8214874267578\n",
            "Epoch 140 train loss. 169.71240234375\n",
            "Epoch 160 train loss. 106.40131378173828\n",
            "Epoch 180 train loss. 94.447021484375\n",
            "Epoch 200 train loss. 148.4458770751953\n",
            "Epoch 220 train loss. 111.57532501220703\n",
            "Epoch 240 train loss. 81.54461669921875\n",
            "Epoch 260 train loss. 87.56201171875\n",
            "Epoch 280 train loss. 65.10360717773438\n",
            "Epoch 300 train loss. 90.55396270751953\n",
            "Epoch 320 train loss. 59.15509796142578\n",
            "Epoch 340 train loss. 45.910343170166016\n",
            "Epoch 360 train loss. 67.6359634399414\n",
            "Epoch 380 train loss. 80.42500305175781\n",
            "Epoch 400 train loss. 87.63569641113281\n",
            "Epoch 420 train loss. 90.84622955322266\n",
            "Epoch 440 train loss. 54.31886291503906\n",
            "Epoch 460 train loss. 53.87678909301758\n",
            "Epoch 480 train loss. 45.49597930908203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK71-QVonUnA",
        "outputId": "77d87215-ce8e-4c67-b6c8-f5e47a68ebe0"
      },
      "source": [
        "with torch.no_grad():\r\n",
        "  X_test = y_train[-1:].unsqueeze(2)\r\n",
        "  pred = model(X_test)\r\n",
        "  pred = torch.flatten(pred)\r\n",
        "  pred = pred.cpu().unsqueeze(1).numpy()\r\n",
        "  score = mean_absolute_percentage_error(test_data, pred)\r\n",
        "  print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.15099968425578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTZdtmQ7pTKh",
        "outputId": "251c6d52-2384-4008-b200-b5062386f6b0"
      },
      "source": [
        "print(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.00730699]\n",
            " [1.00326892]\n",
            " [1.01942121]\n",
            " [1.02903567]\n",
            " [1.03067013]\n",
            " [1.0364388 ]\n",
            " [1.02153639]\n",
            " [1.02442073]\n",
            " [1.02999712]\n",
            " [1.03345832]\n",
            " [1.03441977]\n",
            " [1.03826555]\n",
            " [1.05018748]\n",
            " [1.04711085]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLDJFkIHqH2K",
        "outputId": "663bf0ea-3bb2-4676-8d25-5e24f7263d8e"
      },
      "source": [
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.00166464]\n",
            " [-0.00421416]\n",
            " [-0.00215543]\n",
            " [ 0.00960575]\n",
            " [-0.00517582]\n",
            " [-0.00782692]\n",
            " [-0.00060005]\n",
            " [-0.00514338]\n",
            " [ 0.01110361]\n",
            " [ 0.00467707]\n",
            " [ 0.00264906]\n",
            " [-0.00489027]\n",
            " [-0.00826365]\n",
            " [-0.01006651]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT4cUaDfkjb-",
        "outputId": "b97a4d4b-760e-4809-d48f-c2a173ca344f"
      },
      "source": [
        "# NEED TO IMPLEMENT REVERSE TRANSFORM TO GET ACTUAL PREDICITONS\r\n",
        "# EXAMPLE BELOW:\r\n",
        "# true_cases = scaler.inverse_transform(\r\n",
        "#     np.expand_dims(y_test.flatten().numpy(), axis=0)\r\n",
        "# ).flatten()\r\n",
        "\r\n",
        "# predicted_cases = scaler.inverse_transform(\r\n",
        "#   np.expand_dims(preds, axis=0)\r\n",
        "# ).flatten()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1006, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIgKHP-p3BRo"
      },
      "source": [
        "def make_df_for_stats(row, series):\n",
        "  \"\"\"\n",
        "  Performs summary statistical calculations on a dataframe\n",
        "\n",
        "  Returns: a dictionary with the stats as the value and the stat as the key\n",
        "  \"\"\"\n",
        "\n",
        "  # compute summary statistics\n",
        "  stats = {}\n",
        "  stats['n'] = len(series)\n",
        "  stats['average'] = series.mean()\n",
        "  stats['max'] = series.max()\n",
        "  stats['min'] = series.min()\n",
        "  stats['standard_deviation'] = series.std()\n",
        "\n",
        "  # get the true labels from the test data\n",
        "  true = df_test.loc[f'{row}'].values\n",
        "\n",
        "  # generate profit predictions\n",
        "  fb_prophet_pred = fb_prophet(series, len(true))\n",
        "\n",
        "  # compute and store the mean absolute percentage\n",
        "  stats['fb_prophet_mape'] = mean_absolute_percentage_error(true, fb_prophet_pred)\n",
        "\n",
        "  # return summary statistics and MAPE score\n",
        "  return stats"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}